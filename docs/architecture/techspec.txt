Your intuition is exactly where the industry is heading. For a 3-hour movie or a library of 1,000 videos, you cannot keep everything in a "warm" Context Cache—it would be too expensive and would exceed the  token limits of Flash models.

The strategy you've proposed—**Indexing first, then Caching selected clips**—is known as **Video RAG (Retrieval-Augmented Generation)**. It is the gold standard for "anytime" video analysis.

---

## The "Retrieve-then-Cache" Strategy

Instead of treating the video as one big file, you treat it as a **Searchable Database**.

1. **Permanent Indexing (The Library):** You process every video once, extract "meaning" (embeddings), and store it in a Vector Database (like Qdrant or Pinecone). This lives forever and costs almost nothing to maintain.
2. **Selective Caching (The Chat):** When you ask a question, the system finds the **3–5 most relevant 2-minute clips** across your entire library and *only then* creates a Gemini Context Cache for those specific snippets.

---

## Technical Specification: Video Library Engine (Phase 3)

### 1. The Ingestion Pipeline (One-time Setup)

This is the "heavy lifting" that makes the video searchable "anytime."

* **Chunking:** Split the movie into **30-second to 2-minute "Atomic Scenes"**.
* **Multimodal Embedding:** Use a model (like `multimodal-embedding-001`) to turn each scene into a vector of numbers. This vector captures what the scene *looks* like and *sounds* like.
* **Vector Storage:** Save the vectors + Metadata (Movie Title, Start Time, End Time, Audio Transcript) into **Qdrant**.

### 2. The Search & Chat Logic

This is how the user interacts with the library.

| Step | Action | Logic |
| --- | --- | --- |
| **1. Search** | User asks: *"Where did the main character lose his keys?"* | Search the Vector DB for the top 5 most similar clips. |
| **2. Retrieval** | The DB returns 3 clips from "Movie_A" and 2 from "Movie_B". | Fetch the raw video segments for these specific timestamps. |
| **3. Caching** | Create a temporary **Gemini Context Cache** for those 5 clips. | This allows for high-fidelity chat about just the relevant parts. |
| **4. Response** | Gemini answers: *"He lost them at 01:22:15 in the kitchen scene."* | Provide the answer with clickable timestamps. |

---

## Implementation Instructions for Claude Code

To build this enhancement, Claude Code should implement a **"Library Manager"** class.

### 1. Environment Update

```bash
uv add qdrant-client moviepy google-genai

```

### 2. The "Indexer" Script Requirements

Ask Claude to write a script that does the following:

* **Extract Frames:** Use `OpenCV` or `MoviePy` to take one frame every 2 seconds.
* **Generate Descriptions:** Use **Gemini 3 Flash** (which is 3x faster) to write a 1-sentence summary of every 30-second window.
* **Store in Qdrant:** Upload the summary and the "Video ID" to the vector store.

### 3. The "Hybrid" Streamlit UI

* **Search Mode:** A global search bar that queries the whole library.
* **Analysis Mode:** Once a search result is clicked, it "promotes" that clip into a Chat view (using the Context Caching logic from Phase 2).

---

## Why this is better than "Large Uploads"

* **Cost:** You don't pay to "process" a 3-hour movie every time you open the app. You only pay once for indexing.
* **Speed:** Searching a library of 10,000 hours of video takes **milliseconds** because you are searching numbers in a database, not watching the video.
* **Persistence:** You can close the app and come back a month later; the "index" is still there.

**Would you like me to write the specific "Ingestion Prompt" that Claude should use to generate the scene summaries for your library?**